{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Tic Tac Toe\n",
    "\n",
    "In this lab you will perform deep learning analysis on a dataset of playing [Tic Tac Toe](https://en.wikipedia.org/wiki/Tic-tac-toe).\n",
    "\n",
    "There are 9 grids in Tic Tac Toe that are coded as the following picture shows:\n",
    "\n",
    "![Tic Tac Toe Grids](tttboard.jpg)\n",
    "\n",
    "In the first 9 columns of the dataset you can find which marks (`x` or `o`) exist in the grids. If there is no mark in a certain grid, it is labeled as `b`. The last column is `class` which tells you whether Player X (who always moves first in Tic Tac Toe) wins in this configuration. Note that when `class` has the value `False`, it means either Player O wins the game or it ends up as a draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps suggested below to conduct a neural network analysis using Tensorflow and Keras. You will build a deep learning model to predict whether Player X wins the game or not.\n",
    "\n",
    "## Step 1: Data Engineering\n",
    "\n",
    "This dataset is almost in the ready-to-use state so you do not need to worry about missing values and so on. Still, some simple data engineering is needed.\n",
    "\n",
    "1. Read `tic-tac-toe.csv` into a dataframe.\n",
    "1. Inspect the dataset. Determine if the dataset is reliable by eyeballing the data.\n",
    "1. Convert the categorical values to numeric in all columns.\n",
    "1. Separate the inputs and output.\n",
    "1. Normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TL TM TR ML MM MR BL BM BR  class\n",
      "0  x  x  x  x  o  o  x  o  o   True\n",
      "1  x  x  x  x  o  o  o  x  o   True\n",
      "2  x  x  x  x  o  o  o  o  x   True\n",
      "3  x  x  x  x  o  o  o  b  b   True\n",
      "4  x  x  x  x  o  o  b  o  b   True\n",
      "Shape: (958, 10)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# First, we need to import the pandas library. We give it the nickname 'pd' to type less.\n",
    "import pandas as pd\n",
    "\n",
    "# Now, we read the file. The result is stored in a variable called 'df' (short for DataFrame).\n",
    "df = pd.read_csv('tic-tac-toe.csv')\n",
    "\n",
    "# Let's check if it worked by looking at the first 5 rows of our data.\n",
    "print(df.head())\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tx/45hhw1yn48jfdqn2hmnhbhvw0000gn/T/ipykernel_23578/630537404.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_numeric = df.iloc[:, :9].applymap(mapping.get)\n"
     ]
    }
   ],
   "source": [
    "# We will use a 'dictionary' to define the mapping from letters to numbers.\n",
    "# It's like a real dictionary that translates words.\n",
    "mapping = {'b': 0, 'o': 1, 'x': 2}\n",
    "\n",
    "# Now, we apply this mapping to the first 9 columns.\n",
    "# The .iloc[:, :9] part means \"all rows, first 9 columns\".\n",
    "df_numeric = df.iloc[:, :9].applymap(mapping.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "### 3. Why This Encoding? (0, 1, 2)\n",
    "\n",
    "We used a simple numeric encoding (`b`=0, `o`=1, `x`=2) instead of One-Hot Encoding for a specific reason:\n",
    "\n",
    "*   **Meaningful Order:** The categories here have a natural relationship for our specific goal. The value `2` (X) is the player whose win we are predicting, making it the most significant value. The value `1` (O) is the opponent. The value `0` (blank) is the absence of a player.\n",
    "*   **Preserving Information:** This numeric scale (0, 1, 2) subtly tells the neural network that an `x` is \"greater than\" an `o`, which is \"greater than\" a `b` in the context of predicting X's victory. One-Hot Encoding would have treated all three options as equally separate, losing this useful hint for the model.\n",
    "  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Data:\n",
      "   TL  TM  TR  ML  MM  MR  BL  BM  BR\n",
      "0   2   2   2   2   1   1   2   1   1\n",
      "1   2   2   2   2   1   1   1   2   1\n",
      "2   2   2   2   2   1   1   1   1   2\n",
      "3   2   2   2   2   1   1   1   0   0\n",
      "4   2   2   2   2   1   1   0   1   0\n",
      "\n",
      "Original Class Column:\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "Name: class, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Let's check our work! Print the first 5 rows of the new numeric dataframe.\n",
    "print(\"Numeric Data:\")\n",
    "print(df_numeric.head())\n",
    "\n",
    "# Now, let's also check the 'class' column to see what it looks like.\n",
    "print(\"\\nOriginal Class Column:\")\n",
    "print(df['class'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Separate the inputs and output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (Input features): (958, 9)\n",
      "Shape of y (Target variable): (958,)\n",
      "\n",
      "First 5 rows of y:\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Separate inputs and output\n",
    "# X will be all rows (:) and all columns except the last one (:-1)\n",
    "X = df_numeric.values  # .values converts the DataFrame to a simple NumPy array, which Keras prefers.\n",
    "\n",
    "# y will be all rows (:) and only the last column (-1)\n",
    "# We also use .astype(int) to convert True/False into 1/0. This is required for the next steps.\n",
    "y = df['class'].astype(int).values\n",
    "\n",
    "# Let's check the shapes to make sure it worked!\n",
    "print(\"Shape of X (Input features):\", X.shape)\n",
    "print(\"Shape of y (Target variable):\", y.shape)\n",
    "print(\"\\nFirst 5 rows of y:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of original X: [2 2 2 2 1 1 2 1 1]\n",
      "First row of normalized X: [1.  1.  1.  1.  0.5 0.5 1.  0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the input data (X) by dividing by the maximum value (2)\n",
    "X_normalized = X / 2.0\n",
    "\n",
    "# Let's check the first row to see the result\n",
    "print(\"First row of original X:\", X[0])\n",
    "print(\"First row of normalized X:\", X_normalized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "We normalized the input features (`X`) by dividing all values by the maximum value (2). This transformed the data range from `[0, 1, 2]` to `[0.0, 0.5, 1.0]`.\n",
    "\n",
    "**Why?**\n",
    "Neural networks perform best when all input features are on a **common scale**. Normalization helps the model's internal math work more efficiently, leading to faster training and often better performance. It ensures no single feature (grid cell) dominates the learning process just because it has larger numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Neural Network\n",
    "\n",
    "To build the neural network, you can refer to your own codes you wrote while following the [Deep Learning with Python, TensorFlow, and Keras tutorial](https://www.youtube.com/watch?v=wQ8BIBpya2k) in the lesson. It's pretty similar to what you will be doing in this lab.\n",
    "\n",
    "1. Split the training and test data.\n",
    "1. Create a `Sequential` model.\n",
    "1. Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "1. Compile the model using `adam` as the optimizer and `sparse_categorical_crossentropy` as the loss function. For metrics, use `accuracy` for now.\n",
    "1. Fit the training data.\n",
    "1. Evaluate your neural network model with the test data.\n",
    "1. Save your model as `tic-tac-toe.model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Split the data into Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (766, 9)\n",
      "Testing data shape: (192, 9)\n",
      "Training labels shape: (766,)\n",
      "Testing labels shape: (192,)\n"
     ]
    }
   ],
   "source": [
    "# Import the function we need to split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data (X_normalized is our input, y is our output)\n",
    "# test_size=0.2 means 20% of data is for testing\n",
    "# random_state=42 is like choosing a specific seed for a random number generator.\n",
    "# This ensures we get the same split every time we run the code, which is good for reproducibility.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Let's check the shapes of our new sets\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Testing labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Create a `Sequential` Model\n",
    "\n",
    "In Keras (the library we use with TensorFlow), a Sequential model is like building a layer cake. We add one layer on top of the other, in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty model created. Now we will add layers to it.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# First, we need to import the necessary parts from TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "print(\"Empty model created. Now we will add layers to it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Add Layers to the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf_env/lib/python3.11/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-09-08 14:30:36.746869: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
      "2025-09-08 14:30:36.746897: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-08 14:30:36.746903: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-08 14:30:36.746918: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-08 14:30:36.746928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,666</span> (37.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,666\u001b[0m (37.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,666</span> (37.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,666\u001b[0m (37.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the input and first hidden layer (128 neurons)\n",
    "\n",
    "# 'Dense' means every neuron is connected to every neuron in the next layer.\n",
    "# input_dim=9 defines the input layer (9 features from our board).\n",
    "# units=128 defines how many neurons this layer has. We can start with this number.\n",
    "# activation='relu' is the function that helps the model learn non-linear patterns.\n",
    "model.add(Dense(units=128, activation='relu', input_dim=9))\n",
    "\n",
    "# Add a second hidden layer (64 neurons)\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "# 3. CHANGE THE OUTPUT LAYER for 'sparse_categorical_crossentropy'\n",
    "# We have 2 categories: \"X does not win\" (class 0) and \"X wins\" (class 1)\n",
    "# So we need 2 neurons and softmax activation\n",
    "model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "\n",
    "# Let's see a summary of our model's structure!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully! Ready to train.\n"
     ]
    }
   ],
   "source": [
    "# 4. COMPILE with the instructed loss function\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', # CHANGED\n",
    "              metrics=['accuracy'])\n",
    "print(\"Model compiled successfully! Ready to train.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `compile` step sets the rules for *how* the neural network will learn.\n",
    "\n",
    "*   **Optimizer (`optimizer='adam'`):** This is the algorithm that adjusts the model's internal settings to reduce errors. Think of it as the **engine** that drives the learning process. **Adam** is a very popular and efficient optimizer that works well for most problems.\n",
    "\n",
    "*   **Loss Function (`loss='sparse_categorical_crossentropy'`):** This is how the model calculates its **mistake**. It's a mathematical formula that measures the difference between the model's prediction and the correct answer. This specific function is ideal for classification problems where the outputs are categories (like \"win\" or \"lose\").\n",
    "\n",
    "*   **Metrics (`metrics=['accuracy']`):** This is how we, the humans, evaluate the model's performance. **Accuracy** is the simple percentage of correct predictions, which is easy for us to understand.\n",
    "\n",
    "In short: **The compiler sets up the \"engine\" (Adam), defines how to measure \"mistakes\" (Loss), and how to report \"progress\" (Accuracy).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Fit the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 14:34:10.528781: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6332 - loss: 0.6595 - val_accuracy: 0.6510 - val_loss: 0.6435\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6658 - loss: 0.6247 - val_accuracy: 0.6667 - val_loss: 0.6296\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6567 - loss: 0.6261 - val_accuracy: 0.6406 - val_loss: 0.6409\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6580 - loss: 0.6281 - val_accuracy: 0.6615 - val_loss: 0.6275\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6736 - loss: 0.6207 - val_accuracy: 0.6667 - val_loss: 0.6256\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6802 - loss: 0.6183 - val_accuracy: 0.6719 - val_loss: 0.6251\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6645 - loss: 0.6222 - val_accuracy: 0.6562 - val_loss: 0.6438\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6749 - loss: 0.6196 - val_accuracy: 0.6719 - val_loss: 0.6258\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6762 - loss: 0.6170 - val_accuracy: 0.6823 - val_loss: 0.6260\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6775 - loss: 0.6182 - val_accuracy: 0.6667 - val_loss: 0.6263\n",
      "Model training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test)) # This lets us see test accuracy after each epoch\n",
    "\n",
    "print(\"Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.6: Evaluate the Model with the Test Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6667 - loss: 0.6263 \n",
      "\n",
      "--- Final Evaluation on Test Set ---\n",
      "Test Loss: 0.6263\n",
      "Test Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the results in a nice format\n",
    "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.7: Save your model as `tic-tac-toe.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'tic-tac-toe.model.keras'\n"
     ]
    }
   ],
   "source": [
    "# Save the model using the newer .keras format (recommended)\n",
    "model.save('tic-tac-toe.model.keras') # Just add .keras to the end\n",
    "print(\"Model saved as 'tic-tac-toe.model.keras'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make Predictions\n",
    "\n",
    "Now load your saved model and use it to make predictions on a few random rows in the test dataset. Check if the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
      "\n",
      "Raw predictions (probabilities for class 0 and class 1):\n",
      "[[0.15790036 0.8420996 ]\n",
      " [0.37385976 0.6261403 ]\n",
      " [0.27907187 0.72092813]\n",
      " [0.5589671  0.4410329 ]\n",
      " [0.310625   0.68937504]]\n",
      "\n",
      "Predicted class (0 = X does NOT win, 1 = X wins): [1 1 1 0 1]\n",
      "    True class (0 = X does NOT win, 1 = X wins): [0 1 1 1 1]\n",
      "\n",
      "Checking predictions:\n",
      "Sample 1: Predicted 1, Actual 0 - ✗ WRONG\n",
      "Sample 2: Predicted 1, Actual 1 - ✓ CORRECT\n",
      "Sample 3: Predicted 1, Actual 1 - ✓ CORRECT\n",
      "Sample 4: Predicted 0, Actual 1 - ✗ WRONG\n",
      "Sample 5: Predicted 1, Actual 1 - ✓ CORRECT\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# Step 3: Make Predictions\n",
    "\n",
    "# 1. Load the saved model\n",
    "# (Use the exact same filename you used to save it, including the .keras extension)\n",
    "loaded_model = tf.keras.models.load_model('tic-tac-toe.model.keras')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# 2. Let's select 5 random rows from the test data (X_test)\n",
    "import numpy as np\n",
    "\n",
    "# Get 5 random indices from the test set\n",
    "random_indices = np.random.choice(len(X_test), size=5, replace=False)\n",
    "\n",
    "# Use those indices to get the actual data and labels\n",
    "random_test_samples = X_test[random_indices]\n",
    "random_true_labels = y_test[random_indices]\n",
    "\n",
    "# 3. Use the loaded model to make predictions on these samples\n",
    "# model.predict() returns probabilities for each class [prob_class_0, prob_class_1]\n",
    "predictions = loaded_model.predict(random_test_samples)\n",
    "print(\"\\nRaw predictions (probabilities for class 0 and class 1):\")\n",
    "print(predictions)\n",
    "\n",
    "# 4. Interpret the predictions:\n",
    "# The model outputs probabilities for each class. We take the class with the highest probability.\n",
    "# np.argmax finds the index of the highest value in each prediction.\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"\\nPredicted class (0 = X does NOT win, 1 = X wins):\", predicted_classes)\n",
    "print(\"    True class (0 = X does NOT win, 1 = X wins):\", random_true_labels)\n",
    "\n",
    "# 5. Check if the predictions are correct\n",
    "print(\"\\nChecking predictions:\")\n",
    "for i in range(5):\n",
    "    is_correct = (predicted_classes[i] == random_true_labels[i])\n",
    "    result = \"✓ CORRECT\" if is_correct else \"✗ WRONG\"\n",
    "    print(f\"Sample {i+1}: Predicted {predicted_classes[i]}, Actual {random_true_labels[i]} - {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Improve Your Model\n",
    "\n",
    "Did your model achieve low loss (<0.1) and high accuracy (>0.95)? If not, try to improve your model.\n",
    "\n",
    "But how? There are so many things you can play with in Tensorflow and in the next challenge you'll learn about these things. But in this challenge, let's just do a few things to see if they will help.\n",
    "\n",
    "* Add more layers to your model. If the data are complex you need more layers. But don't use more layers than you need. If adding more layers does not improve the model performance you don't need additional layers.\n",
    "* Adjust the learning rate when you compile the model. This means you will create a custom `tf.keras.optimizers.Adam` instance where you specify the learning rate you want. Then pass the instance to `model.compile` as the optimizer.\n",
    "    * `tf.keras.optimizers.Adam` [reference](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    "    * Don't worry if you don't understand what the learning rate does. You'll learn about it in the next challenge.\n",
    "* Adjust the number of epochs when you fit the training data to the model. Your model performance continues to improve as you train more epochs. But eventually it will reach the ceiling and the performance will stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1:\n",
    "* Add more layers to your model. If the data are complex you need more layers. But don't use more layers than you need. If adding more layers does not improve the model performance you don't need additional layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Improvement 1: Training a Bigger Model ---\n",
      "Epoch 1/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6266 - loss: 0.6393 - val_accuracy: 0.6771 - val_loss: 0.6233\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6606 - loss: 0.6254 - val_accuracy: 0.6771 - val_loss: 0.6221\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6554 - loss: 0.6241 - val_accuracy: 0.6615 - val_loss: 0.6340\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6580 - loss: 0.6353 - val_accuracy: 0.6302 - val_loss: 0.6289\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6345 - loss: 0.6341 - val_accuracy: 0.6146 - val_loss: 0.6317\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6305 - loss: 0.6507 - val_accuracy: 0.6198 - val_loss: 0.6344\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6279 - loss: 0.6402 - val_accuracy: 0.6615 - val_loss: 0.6178\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6475 - loss: 0.6263 - val_accuracy: 0.6510 - val_loss: 0.6288\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6332 - loss: 0.6370 - val_accuracy: 0.6406 - val_loss: 0.6320\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6384 - loss: 0.6430 - val_accuracy: 0.6250 - val_loss: 0.7776\n",
      "Bigger Model Test Accuracy: 0.6250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improvement 1: Bigger Model (More layers/neurons)\n",
    "print(\"--- Improvement 1: Training a Bigger Model ---\")\n",
    "\n",
    "improved_model = Sequential()\n",
    "improved_model.add(Dense(units=256, activation='relu', input_dim=9))\n",
    "improved_model.add(Dense(units=128, activation='relu'))\n",
    "improved_model.add(Dense(units=64, activation='relu'))\n",
    "improved_model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "improved_model.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# Train for 10 epochs (same as before)\n",
    "history_1 = improved_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "test_loss_1, test_accuracy_1 = improved_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Bigger Model Test Accuracy: {test_accuracy_1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2:\n",
    "* Adjust the learning rate when you compile the model. This means you will create a custom `tf.keras.optimizers.Adam` instance where you specify the learning rate you want. Then pass the instance to `model.compile` as the optimizer.\n",
    "    * `tf.keras.optimizers.Adam` [reference](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    "    * Don't worry if you don't understand what the learning rate does. You'll learn about it in the next challenge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Improvement 2: Adjusting Learning Rate ---\n",
      "Epoch 1/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6410 - loss: 0.6491 - val_accuracy: 0.6510 - val_loss: 0.6395\n",
      "Epoch 2/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6593 - loss: 0.6241 - val_accuracy: 0.6562 - val_loss: 0.6297\n",
      "Epoch 3/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6736 - loss: 0.6208 - val_accuracy: 0.6927 - val_loss: 0.6262\n",
      "Epoch 4/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6958 - loss: 0.6177 - val_accuracy: 0.6823 - val_loss: 0.6252\n",
      "Epoch 5/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6867 - loss: 0.6148 - val_accuracy: 0.6875 - val_loss: 0.6252\n",
      "Epoch 6/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6945 - loss: 0.6152 - val_accuracy: 0.6875 - val_loss: 0.6244\n",
      "Epoch 7/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6958 - loss: 0.6144 - val_accuracy: 0.6719 - val_loss: 0.6287\n",
      "Epoch 8/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6723 - loss: 0.6177 - val_accuracy: 0.6667 - val_loss: 0.6244\n",
      "Epoch 9/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6815 - loss: 0.6147 - val_accuracy: 0.6823 - val_loss: 0.6264\n",
      "Epoch 10/10\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6841 - loss: 0.6158 - val_accuracy: 0.6771 - val_loss: 0.6249\n",
      "Adjusted LR Model Test Accuracy: 0.6771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improvement 2: Adjust Learning Rate\n",
    "print(\"--- Improvement 2: Adjusting Learning Rate ---\")\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create a new model (same architecture as the original)\n",
    "lr_model = Sequential()\n",
    "lr_model.add(Dense(units=128, activation='relu', input_dim=9))\n",
    "lr_model.add(Dense(units=64, activation='relu'))\n",
    "lr_model.add(Dense(units=2, activation='softmax'))\n",
    "\n",
    "# Use a custom Adam optimizer with a smaller learning rate\n",
    "custom_adam = Adam(learning_rate=0.0005) # Default is 0.001\n",
    "lr_model.compile(optimizer=custom_adam,\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train for 10 epochs\n",
    "history_2 = lr_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "test_loss_2, test_accuracy_2 = lr_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Adjusted LR Model Test Accuracy: {test_accuracy_2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Improvement 3: Train for More Epochs\n",
    "\n",
    "* Adjust the number of epochs when you fit the training data to the model. Your model performance continues to improve as you train more epochs. But eventually it will reach the ceiling and the performance will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Improvement 3: Training for More Epochs ---\n",
      "Epoch 1/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6057 - loss: 0.7113 - val_accuracy: 0.6458 - val_loss: 0.6694\n",
      "Epoch 2/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6240 - loss: 0.6762 - val_accuracy: 0.5156 - val_loss: 0.7128\n",
      "Epoch 3/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5849 - loss: 0.7532 - val_accuracy: 0.5833 - val_loss: 0.7905\n",
      "Epoch 4/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5666 - loss: 0.8505 - val_accuracy: 0.6250 - val_loss: 0.9155\n",
      "Epoch 5/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5705 - loss: 0.8226 - val_accuracy: 0.6094 - val_loss: 0.8096\n",
      "Epoch 6/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5966 - loss: 0.8028 - val_accuracy: 0.6146 - val_loss: 0.9948\n",
      "Epoch 7/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6175 - loss: 0.8445 - val_accuracy: 0.5938 - val_loss: 1.0788\n",
      "Epoch 8/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6345 - loss: 0.7412 - val_accuracy: 0.5052 - val_loss: 1.1735\n",
      "Epoch 9/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6031 - loss: 0.8085 - val_accuracy: 0.6146 - val_loss: 0.7159\n",
      "Epoch 10/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5927 - loss: 0.8508 - val_accuracy: 0.5208 - val_loss: 0.9014\n",
      "Epoch 11/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5614 - loss: 1.6496 - val_accuracy: 0.5000 - val_loss: 1.5801\n",
      "Epoch 12/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5601 - loss: 1.5496 - val_accuracy: 0.6615 - val_loss: 0.8305\n",
      "Epoch 13/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5862 - loss: 1.3660 - val_accuracy: 0.6510 - val_loss: 2.4515\n",
      "Epoch 14/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5914 - loss: 1.7001 - val_accuracy: 0.6510 - val_loss: 1.0826\n",
      "Epoch 15/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5836 - loss: 1.3076 - val_accuracy: 0.4375 - val_loss: 2.8464\n",
      "Epoch 16/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5692 - loss: 1.3756 - val_accuracy: 0.6510 - val_loss: 1.1435\n",
      "Epoch 17/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5888 - loss: 1.1165 - val_accuracy: 0.4167 - val_loss: 3.0855\n",
      "Epoch 18/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5731 - loss: 2.3511 - val_accuracy: 0.4010 - val_loss: 2.5576\n",
      "Epoch 19/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5705 - loss: 2.3277 - val_accuracy: 0.6094 - val_loss: 2.4570\n",
      "Epoch 20/20\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5653 - loss: 2.0228 - val_accuracy: 0.6250 - val_loss: 1.1279\n",
      "More Epochs Model Test Accuracy: 0.6250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Improvement 3: Train for More Epochs\n",
    "print(\"--- Improvement 3: Training for More Epochs ---\")\n",
    "\n",
    "# Let's take the best model so far and train it longer\n",
    "# We'll use the model from Improvement 1 (bigger model) and train for 10 more epochs (20 total)\n",
    "history_3 = improved_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "test_loss_3, test_accuracy_3 = improved_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"More Epochs Model Test Accuracy: {test_accuracy_3:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which approach(es) did you find helpful to improve your model performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # your answer here\n",
    "\n",
    "# **Answer:**\n",
    "# Based on my experiments, none of the suggested approaches \n",
    "# (adding more layers, adjusting the learning rate, training for more epochs) \n",
    "# significantly improved the model's performance on the test set beyond the original ~67% accuracy.\n",
    "\n",
    "# The best result was achieved by **adjusting the learning rate** (0.0005), \n",
    "# which yielded a test accuracy of **67.71%**, a very minor improvement.\n",
    "\n",
    "# This suggests that a simple neural network might be\n",
    "#  struggling to learn the underlying patterns of Tic Tac Toe wins from this dataset with these features. \n",
    "# The model's performance seems to have a ceiling with this architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
